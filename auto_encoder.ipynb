{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"Using Auto Encoder with Outlier Detection\n", "\"\"\"\n", "# Author: Yue Zhao <zhaoy@cmu.edu>\n", "# License: BSD 2 clause"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import division\n", "from __future__ import print_function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.utils import check_array\n", "from sklearn.utils.validation import check_is_fitted"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from ..utils.utility import check_parameter\n", "from ..utils.stat_models import pairwise_distances_no_broadcast"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from .base import BaseDetector\n", "from .base_dl import _get_tensorflow_version"]}, {"cell_type": "markdown", "metadata": {}, "source": ["if tensorflow 2, import from tf directly"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if _get_tensorflow_version() == 1:\n", "    from keras.models import Sequential\n", "    from keras.layers import Dense, Dropout\n", "    from keras.regularizers import l2\n", "    from keras.losses import mean_squared_error\n", "else:\n", "    from tensorflow.keras.models import Sequential\n", "    from tensorflow.keras.layers import Dense, Dropout\n", "    from tensorflow.keras.regularizers import l2\n", "    from tensorflow.keras.losses import mean_squared_error"]}, {"cell_type": "markdown", "metadata": {}, "source": ["noinspection PyUnresolvedReferences,PyPep8Naming,PyTypeChecker"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class AutoEncoder(BaseDetector):\n", "    \"\"\"Auto Encoder (AE) is a type of neural networks for learning useful data\n", "    representations unsupervisedly. Similar to PCA, AE could be used to\n", "    detect outlying objects in the data by calculating the reconstruction\n", "    errors. See :cite:`aggarwal2015outlier` Chapter 3 for details.\n", "    Parameters\n", "    ----------\n", "    hidden_neurons : list, optional (default=[64, 32, 32, 64])\n", "        The number of neurons per hidden layers.\n", "    hidden_activation : str, optional (default='relu')\n", "        Activation function to use for hidden layers.\n", "        All hidden layers are forced to use the same type of activation.\n", "        See https://keras.io/activations/\n", "    output_activation : str, optional (default='sigmoid')\n", "        Activation function to use for output layer.\n", "        See https://keras.io/activations/\n", "    loss : str or obj, optional (default=keras.losses.mean_squared_error)\n", "        String (name of objective function) or objective function.\n", "        See https://keras.io/losses/\n", "    optimizer : str, optional (default='adam')\n", "        String (name of optimizer) or optimizer instance.\n", "        See https://keras.io/optimizers/\n", "    epochs : int, optional (default=100)\n", "        Number of epochs to train the model.\n", "    batch_size : int, optional (default=32)\n", "        Number of samples per gradient update.\n", "    dropout_rate : float in (0., 1), optional (default=0.2)\n", "        The dropout to be used across all layers.\n", "    l2_regularizer : float in (0., 1), optional (default=0.1)\n", "        The regularization strength of activity_regularizer\n", "        applied on each layer. By default, l2 regularizer is used. See\n", "        https://keras.io/regularizers/\n", "    validation_size : float in (0., 1), optional (default=0.1)\n", "        The percentage of data to be used for validation.\n", "    preprocessing : bool, optional (default=True)\n", "        If True, apply standardization on the data.\n", "    verbose : int, optional (default=1)\n", "        Verbosity mode.\n", "        - 0 = silent\n", "        - 1 = progress bar\n", "        - 2 = one line per epoch.\n", "        For verbose >= 1, model summary may be printed.\n", "    random_state : random_state: int, RandomState instance or None, optional\n", "        (default=None)\n", "        If int, random_state is the seed used by the random\n", "        number generator; If RandomState instance, random_state is the random\n", "        number generator; If None, the random number generator is the\n", "        RandomState instance used by `np.random`.\n", "    contamination : float in (0., 0.5), optional (default=0.1)\n", "        The amount of contamination of the data set, i.e.\n", "        the proportion of outliers in the data set. When fitting this is used\n", "        to define the threshold on the decision function.\n", "    Attributes\n", "    ----------\n", "    encoding_dim_ : int\n", "        The number of neurons in the encoding layer.\n", "    compression_rate_ : float\n", "        The ratio between the original feature and\n", "        the number of neurons in the encoding layer.\n", "    model_ : Keras Object\n", "        The underlying AutoEncoder in Keras.\n", "    history_: Keras Object\n", "        The AutoEncoder training history.\n", "    decision_scores_ : numpy array of shape (n_samples,)\n", "        The outlier scores of the training data.\n", "        The higher, the more abnormal. Outliers tend to have higher\n", "        scores. This value is available once the detector is\n", "        fitted.\n", "    threshold_ : float\n", "        The threshold is based on ``contamination``. It is the\n", "        ``n_samples * contamination`` most abnormal samples in\n", "        ``decision_scores_``. The threshold is calculated for generating\n", "        binary outlier labels.\n", "    labels_ : int, either 0 or 1\n", "        The binary labels of the training data. 0 stands for inliers\n", "        and 1 for outliers/anomalies. It is generated by applying\n", "        ``threshold_`` on ``decision_scores_``.\n", "    \"\"\"\n", "    def __init__(self, hidden_neurons=None,\n", "                 hidden_activation='relu', output_activation='sigmoid',\n", "                 loss=mean_squared_error, optimizer='adam',\n", "                 epochs=100, batch_size=32, dropout_rate=0.2,\n", "                 l2_regularizer=0.1, validation_size=0.1, preprocessing=True,\n", "                 verbose=1, random_state=None, contamination=0.1):\n", "        super(AutoEncoder, self).__init__(contamination=contamination)\n", "        self.hidden_neurons = hidden_neurons\n", "        self.hidden_activation = hidden_activation\n", "        self.output_activation = output_activation\n", "        self.loss = loss\n", "        self.optimizer = optimizer\n", "        self.epochs = epochs\n", "        self.batch_size = batch_size\n", "        self.dropout_rate = dropout_rate\n", "        self.l2_regularizer = l2_regularizer\n", "        self.validation_size = validation_size\n", "        self.preprocessing = preprocessing\n", "        self.verbose = verbose\n", "        self.random_state = random_state\n\n", "        # default values\n", "        if self.hidden_neurons is None:\n", "            self.hidden_neurons = [64, 32, 32, 64]\n\n", "        # Verify the network design is valid\n", "        if not self.hidden_neurons == self.hidden_neurons[::-1]:\n", "            print(self.hidden_neurons)\n", "            raise ValueError(\"Hidden units should be symmetric\")\n", "        self.hidden_neurons_ = self.hidden_neurons\n", "        check_parameter(dropout_rate, 0, 1, param_name='dropout_rate',\n", "                        include_left=True)\n", "    def _build_model(self):\n", "        model = Sequential()\n", "        # Input layer\n", "        model.add(Dense(\n", "            self.hidden_neurons_[0], activation=self.hidden_activation,\n", "            input_shape=(self.n_features_,),\n", "            activity_regularizer=l2(self.l2_regularizer)))\n", "        model.add(Dropout(self.dropout_rate))\n\n", "        # Additional layers\n", "        for i, hidden_neurons in enumerate(self.hidden_neurons_, 1):\n", "            model.add(Dense(\n", "                hidden_neurons,\n", "                activation=self.hidden_activation,\n", "                activity_regularizer=l2(self.l2_regularizer)))\n", "            model.add(Dropout(self.dropout_rate))\n\n", "        # Output layers\n", "        model.add(Dense(self.n_features_, activation=self.output_activation,\n", "                        activity_regularizer=l2(self.l2_regularizer)))\n\n", "        # Compile model\n", "        model.compile(loss=self.loss, optimizer=self.optimizer)\n", "        if self.verbose >= 1:\n", "            print(model.summary())\n", "        return model\n\n", "    # noinspection PyUnresolvedReferences\n", "    def fit(self, X, y=None):\n", "        \"\"\"Fit detector. y is ignored in unsupervised methods.\n", "        Parameters\n", "        ----------\n", "        X : numpy array of shape (n_samples, n_features)\n", "            The input samples.\n", "        y : Ignored\n", "            Not used, present for API consistency by convention.\n", "        Returns\n", "        -------\n", "        self : object\n", "            Fitted estimator.\n", "        \"\"\"\n", "        # validate inputs X and y (optional)\n", "        X = check_array(X)\n", "        self._set_n_classes(y)\n\n", "        # Verify and construct the hidden units\n", "        self.n_samples_, self.n_features_ = X.shape[0], X.shape[1]\n\n", "        # Standardize data for better performance\n", "        if self.preprocessing:\n", "            self.scaler_ = StandardScaler()\n", "            X_norm = self.scaler_.fit_transform(X)\n", "        else:\n", "            X_norm = np.copy(X)\n\n", "        # Shuffle the data for validation as Keras do not shuffling for\n", "        # Validation Split\n", "        np.random.shuffle(X_norm)\n\n", "        # Validate and complete the number of hidden neurons\n", "        if np.min(self.hidden_neurons) > self.n_features_:\n", "            raise ValueError(\"The number of neurons should not exceed \"\n", "                             \"the number of features\")\n", "        self.hidden_neurons_.insert(0, self.n_features_)\n\n", "        # Calculate the dimension of the encoding layer & compression rate\n", "        self.encoding_dim_ = np.median(self.hidden_neurons)\n", "        self.compression_rate_ = self.n_features_ // self.encoding_dim_\n\n", "        # Build AE model & fit with X\n", "        self.model_ = self._build_model()\n", "        self.history_ = self.model_.fit(X_norm, X_norm,\n", "                                        epochs=self.epochs,\n", "                                        batch_size=self.batch_size,\n", "                                        shuffle=True,\n", "                                        validation_split=self.validation_size,\n", "                                        verbose=self.verbose).history\n", "        # Reverse the operation for consistency\n", "        self.hidden_neurons_.pop(0)\n", "        # Predict on X itself and calculate the reconstruction error as\n", "        # the outlier scores. Noted X_norm was shuffled has to recreate\n", "        if self.preprocessing:\n", "            X_norm = self.scaler_.transform(X)\n", "        else:\n", "            X_norm = np.copy(X)\n", "        pred_scores = self.model_.predict(X_norm)\n", "        self.decision_scores_ = pairwise_distances_no_broadcast(X_norm,\n", "                                                                pred_scores)\n", "        self._process_decision_scores()\n", "        return self\n", "    def decision_function(self, X):\n", "        \"\"\"Predict raw anomaly score of X using the fitted detector.\n", "        The anomaly score of an input sample is computed based on different\n", "        detector algorithms. For consistency, outliers are assigned with\n", "        larger anomaly scores.\n", "        Parameters\n", "        ----------\n", "        X : numpy array of shape (n_samples, n_features)\n", "            The training input samples. Sparse matrices are accepted only\n", "            if they are supported by the base estimator.\n", "        Returns\n", "        -------\n", "        anomaly_scores : numpy array of shape (n_samples,)\n", "            The anomaly score of the input samples.\n", "        \"\"\"\n", "        check_is_fitted(self, ['model_', 'history_'])\n", "        X = check_array(X)\n", "        if self.preprocessing:\n", "            X_norm = self.scaler_.transform(X)\n", "        else:\n", "            X_norm = np.copy(X)\n\n", "        # Predict on X and return the reconstruction errors\n", "        pred_scores = self.model_.predict(X_norm)\n", "        return pairwise_distances_no_broadcast(X_norm, pred_scores)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}