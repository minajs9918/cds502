{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nVariational Auto Encoder (VAE)<br>\n", "and beta-VAE for Unsupervised Outlier Detection<br>\n", "Reference:<br>\n", "        :cite:`kingma2013auto` Kingma, Diederik, Welling<br>\n", "        'Auto-Encodeing Variational Bayes'<br>\n", "        https://arxiv.org/abs/1312.6114<br>\n", "        <br>\n", "        :cite:`burgess2018understanding` Burges et al<br>\n", "        'Understanding disentangling in beta-VAE'<br>\n", "        https://arxiv.org/pdf/1804.03599.pdf<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Author: Andrij Vasylenko <andrij@liverpool.ac.uk><br>\n", "License: BSD 2 clause"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import division\n", "from __future__ import print_function\n", "from __future__ import absolute_import"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\n", "from sklearn.utils import check_array\n", "from sklearn.utils.validation import check_is_fitted"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from ..utils.utility import check_parameter\n", "from ..utils.stat_models import pairwise_distances_no_broadcast"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from .base import BaseDetector\n", "from .base_dl import _get_tensorflow_version"]}, {"cell_type": "markdown", "metadata": {}, "source": ["if tensorflow 2, import from tf directly"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if _get_tensorflow_version() == 1:\n", "    from keras.models import Model\n", "    from keras.layers import Lambda, Input, Dense, Dropout\n", "    from keras.regularizers import l2\n", "    from keras.losses import mse, binary_crossentropy\n", "    from keras import backend as K\n", "else:\n", "    from tensorflow.keras.models import Model\n", "    from tensorflow.keras.layers import Lambda, Input, Dense, Dropout\n", "    from tensorflow.keras.regularizers import l2\n", "    from tensorflow.keras.losses import mse, binary_crossentropy\n", "    from tensorflow.keras import backend as K"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class VAE(BaseDetector):\n", "    \"\"\" Variational auto encoder\n", "    Encoder maps X onto a latent space Z\n", "    Decoder samples Z from N(0,1)\n", "    VAE_loss = Reconstruction_loss + KL_loss\n", "    Reference\n", "    See :cite:`kingma2013auto` Kingma, Diederik, Welling\n", "    'Auto-Encodeing Variational Bayes'\n", "    https://arxiv.org/abs/1312.6114 for details.\n", "    beta VAE\n", "    In Loss, the emphasis is on KL_loss\n", "    and capacity of a bottleneck:\n", "    VAE_loss = Reconstruction_loss + gamma*KL_loss\n", "    Reference\n", "    See :cite:`burgess2018understanding` Burges et al\n", "    'Understanding disentangling in beta-VAE'\n", "    https://arxiv.org/pdf/1804.03599.pdf for details."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    Parameters\n", "    ----------\n", "    encoder_neurons : list, optional (default=[128, 64, 32])\n", "        The number of neurons per hidden layer in encoder.\n", "    decoder_neurons : list, optional (default=[32, 64, 128])\n", "        The number of neurons per hidden layer in decoder.\n", "    hidden_activation : str, optional (default='relu')\n", "        Activation function to use for hidden layers.\n", "        All hidden layers are forced to use the same type of activation.\n", "        See https://keras.io/activations/\n", "    output_activation : str, optional (default='sigmoid')\n", "        Activation function to use for output layer.\n", "        See https://keras.io/activations/\n", "    loss : str or obj, optional (default=keras.losses.mean_squared_error\n", "        String (name of objective function) or objective function.\n", "        See https://keras.io/losses/\n", "    gamma : float, optional (default=1.0)\n", "        Coefficient of beta VAE regime.\n", "        Default is regular VAE.\n", "    capacity : float, optional (default=0.0)\n", "        Maximum capacity of a loss bottle neck.\n", "    optimizer : str, optional (default='adam')\n", "        String (name of optimizer) or optimizer instance.\n", "        See https://keras.io/optimizers/\n", "    epochs : int, optional (default=100)\n", "        Number of epochs to train the model.\n", "    batch_size : int, optional (default=32)\n", "        Number of samples per gradient update.\n", "    dropout_rate : float in (0., 1), optional (default=0.2)\n", "        The dropout to be used across all layers.\n", "    l2_regularizer : float in (0., 1), optional (default=0.1)\n", "        The regularization strength of activity_regularizer\n", "        applied on each layer. By default, l2 regularizer is used. See\n", "        https://keras.io/regularizers/\n", "    validation_size : float in (0., 1), optional (default=0.1)\n", "        The percentage of data to be used for validation.\n", "    preprocessing : bool, optional (default=True)\n", "        If True, apply standardization on the data.\n", "    verbose : int, optional (default=1)\n", "        verbose mode.\n", "        - 0 = silent\n", "        - 1 = progress bar\n", "        - 2 = one line per epoch.\n", "        For verbose >= 1, model summary may be printed.\n", "    random_state : random_state: int, RandomState instance or None, opti\n", "        (default=None)\n", "        If int, random_state is the seed used by the random\n", "        number generator; If RandomState instance, random_state is the r\n", "        number generator; If None, the random number generator is the\n", "        RandomState instance used by `np.random`.\n", "    contamination : float in (0., 0.5), optional (default=0.1)\n", "        The amount of contamination of the data set, i.e.\n", "        the proportion of outliers in the data set. When fitting this is\n", "        to define the threshold on the decision function.\n", "    Attributes\n", "    ----------\n", "    encoding_dim_ : int\n", "        The number of neurons in the encoding layer.\n", "    compression_rate_ : float\n", "        The ratio between the original feature and\n", "        the number of neurons in the encoding layer.\n", "    model_ : Keras Object\n", "        The underlying AutoEncoder in Keras.\n", "    history_: Keras Object\n", "        The AutoEncoder training history.\n", "    decision_scores_ : numpy array of shape (n_samples,)\n", "        The outlier scores of the training data.\n", "        The higher, the more abnormal. Outliers tend to have higher\n", "        scores. This value is available once the detector is\n", "        fitted.\n", "    threshold_ : float\n", "        The threshold is based on ``contamination``. It is the\n", "        ``n_samples * contamination`` most abnormal samples in\n", "        ``decision_scores_``. The threshold is calculated for generating\n", "        binary outlier labels.\n", "    labels_ : int, either 0 or 1\n", "        The binary labels of the training data. 0 stands for inliers\n", "        and 1 for outliers/anomalies. It is generated by applying\n", "        ``threshold_`` on ``decision_scores_``.\n", "    \"\"\"\n", "    def __init__(self, encoder_neurons=None, decoder_neurons=None,\n", "                 latent_dim=2, hidden_activation='relu',\n", "                 output_activation='sigmoid', loss=mse, optimizer='adam',\n", "                 epochs=100, batch_size=32, dropout_rate=0.2,\n", "                 l2_regularizer=0.1, validation_size=0.1, preprocessing=True,\n", "                 verbose=1, random_state=None, contamination=0.1,\n", "                 gamma=1.0, capacity=0.0):\n", "        super(VAE, self).__init__(contamination=contamination)\n", "        self.encoder_neurons = encoder_neurons\n", "        self.decoder_neurons = decoder_neurons\n", "        self.hidden_activation = hidden_activation\n", "        self.output_activation = output_activation\n", "        self.loss = loss\n", "        self.optimizer = optimizer\n", "        self.epochs = epochs\n", "        self.batch_size = batch_size\n", "        self.dropout_rate = dropout_rate\n", "        self.l2_regularizer = l2_regularizer\n", "        self.validation_size = validation_size\n", "        self.preprocessing = preprocessing\n", "        self.verbose = verbose\n", "        self.random_state = random_state\n", "        self.latent_dim = latent_dim\n", "        self.gamma = gamma\n", "        self.capacity = capacity\n\n", "        # default values\n", "        if self.encoder_neurons is None:\n", "            self.encoder_neurons = [128, 64, 32]\n", "        if self.decoder_neurons is None:\n", "            self.decoder_neurons = [32, 64, 128]\n", "        self.encoder_neurons_ = self.encoder_neurons\n", "        self.decoder_neurons_ = self.decoder_neurons\n", "        check_parameter(dropout_rate, 0, 1, param_name='dropout_rate',\n", "                        include_left=True)\n", "    def sampling(self, args):\n", "        \"\"\"Reparametrisation by sampling from Gaussian, N(0,I)\n", "        To sample from epsilon = Norm(0,I) instead of from likelihood Q(z|X)\n", "        with latent variables z: z = z_mean + sqrt(var) * epsilon\n", "        Parameters\n", "        ----------\n", "        args : tensor\n", "            Mean and log of variance of Q(z|X).\n", "    \n", "        Returns\n", "        -------\n", "        z : tensor\n", "            Sampled latent variable.\n", "        \"\"\"\n", "        z_mean, z_log = args\n", "        batch = K.shape(z_mean)[0]  # batch size\n", "        dim = K.int_shape(z_mean)[1]  # latent dimension\n", "        epsilon = K.random_normal(shape=(batch, dim))  # mean=0, std=1.0\n", "        return z_mean + K.exp(0.5 * z_log) * epsilon\n", "    def vae_loss(self, inputs, outputs, z_mean, z_log):\n", "        \"\"\" Loss = Recreation loss + Kullback-Leibler loss\n", "        for probability function divergence (ELBO).\n", "        gamma > 1 and capacity != 0 for beta-VAE\n", "        \"\"\"\n", "        reconstruction_loss = self.loss(inputs, outputs)\n", "        reconstruction_loss *= self.n_features_\n", "        kl_loss = 1 + z_log - K.square(z_mean) - K.exp(z_log)\n", "        kl_loss = -0.5 * K.sum(kl_loss, axis=-1)\n", "        kl_loss = self.gamma * K.abs(kl_loss - self.capacity)\n", "        return K.mean(reconstruction_loss + kl_loss)\n", "    def _build_model(self):\n", "        \"\"\"Build VAE = encoder + decoder + vae_loss\"\"\"\n\n", "        # Build Encoder\n", "        inputs = Input(shape=(self.n_features_,))\n", "        # Input layer\n", "        layer = Dense(self.n_features_, activation=self.hidden_activation)(\n", "            inputs)\n", "        # Hidden layers\n", "        for neurons in self.encoder_neurons:\n", "            layer = Dense(neurons, activation=self.hidden_activation,\n", "                          activity_regularizer=l2(self.l2_regularizer))(layer)\n", "            layer = Dropout(self.dropout_rate)(layer)\n", "        # Create mu and sigma of latent variables\n", "        z_mean = Dense(self.latent_dim)(layer)\n", "        z_log = Dense(self.latent_dim)(layer)\n", "        # Use parametrisation sampling\n", "        z = Lambda(self.sampling, output_shape=(self.latent_dim,))(\n", "            [z_mean, z_log])\n", "        # Instantiate encoder\n", "        encoder = Model(inputs, [z_mean, z_log, z])\n", "        if self.verbose >= 1:\n", "            encoder.summary()\n\n", "        # Build Decoder\n", "        latent_inputs = Input(shape=(self.latent_dim,))\n", "        # Latent input layer\n", "        layer = Dense(self.latent_dim, activation=self.hidden_activation)(\n", "            latent_inputs)\n", "        # Hidden layers\n", "        for neurons in self.decoder_neurons:\n", "            layer = Dense(neurons, activation=self.hidden_activation)(layer)\n", "            layer = Dropout(self.dropout_rate)(layer)\n", "        # Output layer\n", "        outputs = Dense(self.n_features_, activation=self.output_activation)(\n", "            layer)\n", "        # Instatiate decoder\n", "        decoder = Model(latent_inputs, outputs)\n", "        if self.verbose >= 1:\n", "            decoder.summary()\n", "        # Generate outputs\n", "        outputs = decoder(encoder(inputs)[2])\n\n", "        # Instantiate VAE\n", "        vae = Model(inputs, outputs)\n", "        vae.add_loss(self.vae_loss(inputs, outputs, z_mean, z_log))\n", "        vae.compile(optimizer=self.optimizer)\n", "        if self.verbose >= 1:\n", "            vae.summary()\n", "        return vae\n", "    def fit(self, X, y=None):\n", "        \"\"\"Fit detector. y is optional for unsupervised methods.\n", "        Parameters\n", "        ----------\n", "        X : numpy array of shape (n_samples, n_features)\n", "            The input samples.\n", "        y : numpy array of shape (n_samples,), optional (default=None)\n", "            The ground truth of the input samples (labels).\n", "        \"\"\"\n", "        # validate inputs X and y (optional)\n", "        X = check_array(X)\n", "        self._set_n_classes(y)\n\n", "        # Verify and construct the hidden units\n", "        self.n_samples_, self.n_features_ = X.shape[0], X.shape[1]\n\n", "        # Standardize data for better performance\n", "        if self.preprocessing:\n", "            self.scaler_ = StandardScaler()\n", "            X_norm = self.scaler_.fit_transform(X)\n", "        else:\n", "            X_norm = np.copy(X)\n\n", "        # Shuffle the data for validation as Keras do not shuffling for\n", "        # Validation Split\n", "        np.random.shuffle(X_norm)\n\n", "        # Validate and complete the number of hidden neurons\n", "        if np.min(self.encoder_neurons) > self.n_features_:\n", "            raise ValueError(\"The number of neurons should not exceed \"\n", "                             \"the number of features\")\n\n", "        # Build VAE model & fit with X\n", "        self.model_ = self._build_model()\n", "        self.history_ = self.model_.fit(X_norm,\n", "                                        epochs=self.epochs,\n", "                                        batch_size=self.batch_size,\n", "                                        shuffle=True,\n", "                                        validation_split=self.validation_size,\n", "                                        verbose=self.verbose).history\n", "        # Predict on X itself and calculate the reconstruction error as\n", "        # the outlier scores. Noted X_norm was shuffled has to recreate\n", "        if self.preprocessing:\n", "            X_norm = self.scaler_.transform(X)\n", "        else:\n", "            X_norm = np.copy(X)\n", "        pred_scores = self.model_.predict(X_norm)\n", "        self.decision_scores_ = pairwise_distances_no_broadcast(X_norm,\n", "                                                                pred_scores)\n", "        self._process_decision_scores()\n", "        return self\n", "    def decision_function(self, X):\n", "        \"\"\"Predict raw anomaly score of X using the fitted detector.\n", "        The anomaly score of an input sample is computed based on different\n", "        detector algorithms. For consistency, outliers are assigned with\n", "        larger anomaly scores.\n", "        Parameters\n", "        ----------\n", "        X : numpy array of shape (n_samples, n_features)\n", "            The training input samples. Sparse matrices are accepted only\n", "            if they are supported by the base estimator.\n", "        Returns\n", "        -------\n", "        anomaly_scores : numpy array of shape (n_samples,)\n", "            The anomaly score of the input samples.\n", "        \"\"\"\n", "        check_is_fitted(self, ['model_', 'history_'])\n", "        X = check_array(X)\n", "        if self.preprocessing:\n", "            X_norm = self.scaler_.transform(X)\n", "        else:\n", "            X_norm = np.copy(X)\n\n", "        # Predict on X and return the reconstruction errors\n", "        pred_scores = self.model_.predict(X_norm)\n", "        return pairwise_distances_no_broadcast(X_norm, pred_scores)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}