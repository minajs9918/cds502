{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aiml4it/anaconda/3-5.2.0-generic/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/aiml4it/anaconda/3-5.2.0-generic/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/opt/aiml4it/anaconda/3-5.2.0-generic/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/aiml4it/anaconda/3-5.2.0-generic/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/opt/aiml4it/anaconda/3-5.2.0-generic/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/aiml4it/anaconda/3-5.2.0-generic/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from scipy.misc import imsave\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.utils import plot_model\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 6\n",
    "max_iterations = 6*6\n",
    "batch_size = 10\n",
    "original_dim = 36\n",
    "intermediate_dim = 13\n",
    "latent_dim = 10\n",
    "epochs = 100\n",
    "input_shape = (original_dim, )\n",
    "epsilon_std = 0.3\n",
    "beta = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recur_divide_data(x,i,h,w):\n",
    "    if i <= max_iterations:\n",
    "        if h != 0 or w != 0 :\n",
    "            random_split = random.randint(0,1) # choose between horizontally and vertically\n",
    "            random_pos = random.randint(0,min(h,w)) # it starts from 1 otherwise we will have a lot of grids with just value 0 all over it            \n",
    "            #print('random_split', random_split)\n",
    "            #print('random_pos', random_pos)\n",
    "            if random_split == 0 :# vertically case\n",
    "                #fill the cells\n",
    "                x[:h,random_pos:w] = i\n",
    "                #print('x',x)\n",
    "                return recur_divide_data(x,i+1,h,random_pos)\n",
    "            else:\n",
    "                #fill the cells\n",
    "                x[random_pos:h,:w] = i\n",
    "                #print('x',x)\n",
    "                return recur_divide_data(x,i+1,random_pos,w)\n",
    "        else:\n",
    "            #print('when h =0 and w = 0 i=', i)\n",
    "            #x[h,w] = i\n",
    "            return\n",
    "    else:\n",
    "        return\n",
    "    #if (x == np.zeros(shape=(DIM,DIM))) and (i == 3): # if in 3 attemtps there were the position is always zero\n",
    "     #   x = np.array([[0,0,0,0,0,0],[0,0,0,0,0,0],[0,0,0,0,0,0],[1,1,1,1,1,1],[1,1,1,1,1,1],[1,1,1,1,1,1]])\n",
    "     #   return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_set(dim):\n",
    "    x_total = []\n",
    "    x_unique = []\n",
    "    #since we remove the case where it is filled with one value, we have to add it manually\n",
    "    x_total.append(np.zeros(shape=(dim,dim)).flatten())\n",
    "    for i in range(30000):\n",
    "        x = np.zeros(shape=(dim,dim))\n",
    "        recur_divide_data(x,0,dim,dim)\n",
    "        #if ((x != np.zeros(shape=(dim,dim))).all()) and ((x != np.ones(shape=(dim,dim))).all()):\n",
    "        x_total.append(x.flatten())\n",
    "    x_total = np.array(x_total)\n",
    "    #to get only unique values out of the data\n",
    "    unique_rows = np.unique(x_total, axis=0)\n",
    "    return unique_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_training_set(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4524, 36)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 2. 2. 2. 1. 0. 3. 2. 2. 2. 1. 0. 3. 2. 2. 2. 1. 0. 3. 2. 2. 2. 1. 0.\n",
      " 3. 2. 2. 2. 1. 0. 3. 2. 2. 2. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0ef130d668>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x[500])\n",
    "plt.imshow(x[40].reshape(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(x_dim):\n",
    "    x_r = np.ndarray(shape=(x_dim,x_dim))\n",
    "    step_x = random.randint(1,x_dim -2) \n",
    "    step_y = random.randint(1,x_dim - 2)\n",
    "    for i in range(0,x_dim, step_x):\n",
    "        for j in range(0,x_dim, step_y):\n",
    "            #in every step generate a new random integer\n",
    "            rand = random.randint(0,37)\n",
    "            x_r[i:i+step_x,j:j+step_y] = rand \n",
    "    return x_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_randomly_data():\n",
    "    reshaped_vec= []\n",
    "    for i in range(30000):\n",
    "        vec = generate_data(6)\n",
    "        vec_t = np.transpose(vec)\n",
    "        vec = vec.flatten()\n",
    "        vec_t = vec_t.flatten()\n",
    "        reshaped_vec.append(vec)\n",
    "        reshaped_vec.append(vec_t)\n",
    "    return np.array(reshaped_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_randomly_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the training index so it can be divided by the batch size so we won't have errors in training\n",
    "if x.shape[0] % batch_size != 0:\n",
    "    training_index = int(x.shape[0] / batch_size)*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[0:training_index-1000,]\n",
    "x_test = x[training_index-1000:training_index,]\n",
    "x_train = x_train.astype('float32') / 36.\n",
    "x_test = x_test.astype('float32') / 36."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the latent representation vectors !!\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(models,\n",
    "\n",
    "                 data,\n",
    "\n",
    "                 batch_size=20,\n",
    "\n",
    "                 model_name=\"vae\"):\n",
    "\n",
    "    encoder, decoder = models\n",
    "\n",
    "    x_test, _ = data\n",
    "\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,batch_size=batch_size)\n",
    "    x_decoded = decoder.predict(z_mean)\n",
    "    \n",
    "    combined_data = np.array([x_test,x_decoded])\n",
    "    #Get the min and max of all your data\n",
    "    _min, _max = np.amin(combined_data), np.amax(combined_data)\n",
    "    \n",
    "    #Try with my own z_sample dimensions = (1, latent_dimension)!!!\n",
    "    z_mean_test = np.array([[0,0,0,0,0,0,0,0,0,0]])\n",
    "    x_decoded_test = decoder.predict(z_mean_test)\n",
    "    print('x_decoded_test ', x_decoded_test)\n",
    "    plt.imshow(x_decoded_test.reshape(DIM, DIM), cmap=plt.cm.hsv, vmin = _min, vmax = _max)\n",
    "    \n",
    "    n = 10  # how many digits we will display\n",
    "    plt.figure(figsize=(40, 8))\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(x_test[i].reshape(DIM, DIM), cmap=plt.cm.hsv, vmin = _min, vmax = _max)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(x_decoded[i].reshape(DIM, DIM), cmap=plt.cm.hsv, vmin = _min, vmax = _max)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "\n",
    "# build encoder model\n",
    "\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use reparameterization trick to push the sampling out as input\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 13)           481         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 10)           140         dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 10)           140         dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 10)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 761\n",
      "Trainable params: 761\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate encoder model\n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "encoder.summary()\n",
    "\n",
    "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build decoder model\n",
    "\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 13)                143       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 36)                504       \n",
      "=================================================================\n",
      "Total params: 647\n",
      "Trainable params: 647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate decoder model\n",
    "\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "decoder.summary()\n",
    "\n",
    "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 10), (None, 10),  761       \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 36)                647       \n",
      "=================================================================\n",
      "Total params: 1,408\n",
      "Trainable params: 1,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3520 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "3520/3520 [==============================] - 1s 319us/step - loss: 11.6617 - val_loss: 8.1180\n",
      "Epoch 2/100\n",
      "3520/3520 [==============================] - 0s 118us/step - loss: 6.8142 - val_loss: 7.6576\n",
      "Epoch 3/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.7173 - val_loss: 7.5167\n",
      "Epoch 4/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.6900 - val_loss: 7.4697\n",
      "Epoch 5/100\n",
      "3520/3520 [==============================] - 0s 124us/step - loss: 6.6773 - val_loss: 7.4400\n",
      "Epoch 6/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.6676 - val_loss: 7.4210\n",
      "Epoch 7/100\n",
      "3520/3520 [==============================] - 0s 123us/step - loss: 6.6531 - val_loss: 7.3947\n",
      "Epoch 8/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.6173 - val_loss: 7.3422\n",
      "Epoch 9/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.5691 - val_loss: 7.3078\n",
      "Epoch 10/100\n",
      "3520/3520 [==============================] - 0s 128us/step - loss: 6.5483 - val_loss: 7.2783\n",
      "Epoch 11/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.5352 - val_loss: 7.2651\n",
      "Epoch 12/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.5234 - val_loss: 7.2461\n",
      "Epoch 13/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.5132 - val_loss: 7.2329\n",
      "Epoch 14/100\n",
      "3520/3520 [==============================] - 0s 123us/step - loss: 6.5037 - val_loss: 7.2107\n",
      "Epoch 15/100\n",
      "3520/3520 [==============================] - 0s 118us/step - loss: 6.4927 - val_loss: 7.2032\n",
      "Epoch 16/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.4807 - val_loss: 7.1941\n",
      "Epoch 17/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.4688 - val_loss: 7.1836\n",
      "Epoch 18/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.4585 - val_loss: 7.1810\n",
      "Epoch 19/100\n",
      "3520/3520 [==============================] - 0s 138us/step - loss: 6.4479 - val_loss: 7.1753\n",
      "Epoch 20/100\n",
      "3520/3520 [==============================] - 0s 141us/step - loss: 6.4405 - val_loss: 7.1629\n",
      "Epoch 21/100\n",
      "3520/3520 [==============================] - 1s 147us/step - loss: 6.4356 - val_loss: 7.1752\n",
      "Epoch 22/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.4316 - val_loss: 7.1591\n",
      "Epoch 23/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.4282 - val_loss: 7.1628\n",
      "Epoch 24/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.4248 - val_loss: 7.1529\n",
      "Epoch 25/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.4212 - val_loss: 7.1577\n",
      "Epoch 26/100\n",
      "3520/3520 [==============================] - 0s 127us/step - loss: 6.4169 - val_loss: 7.1380\n",
      "Epoch 27/100\n",
      "3520/3520 [==============================] - 0s 126us/step - loss: 6.4126 - val_loss: 7.1398\n",
      "Epoch 28/100\n",
      "3520/3520 [==============================] - 0s 126us/step - loss: 6.4093 - val_loss: 7.1342\n",
      "Epoch 29/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.4062 - val_loss: 7.1284\n",
      "Epoch 30/100\n",
      "3520/3520 [==============================] - 0s 124us/step - loss: 6.4029 - val_loss: 7.1249\n",
      "Epoch 31/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.4000 - val_loss: 7.1202\n",
      "Epoch 32/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.3966 - val_loss: 7.1121\n",
      "Epoch 33/100\n",
      "3520/3520 [==============================] - 0s 129us/step - loss: 6.3934 - val_loss: 7.1105\n",
      "Epoch 34/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3904 - val_loss: 7.1121\n",
      "Epoch 35/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.3878 - val_loss: 7.1007\n",
      "Epoch 36/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3851 - val_loss: 7.1074\n",
      "Epoch 37/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.3828 - val_loss: 7.1023\n",
      "Epoch 38/100\n",
      "3520/3520 [==============================] - 0s 118us/step - loss: 6.3810 - val_loss: 7.1046\n",
      "Epoch 39/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.3795 - val_loss: 7.0893\n",
      "Epoch 40/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.3778 - val_loss: 7.0915\n",
      "Epoch 41/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.3763 - val_loss: 7.0896\n",
      "Epoch 42/100\n",
      "3520/3520 [==============================] - 0s 123us/step - loss: 6.3751 - val_loss: 7.0917\n",
      "Epoch 43/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3737 - val_loss: 7.0874\n",
      "Epoch 44/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.3724 - val_loss: 7.0901\n",
      "Epoch 45/100\n",
      "3520/3520 [==============================] - 0s 137us/step - loss: 6.3712 - val_loss: 7.0807\n",
      "Epoch 46/100\n",
      "3520/3520 [==============================] - 1s 152us/step - loss: 6.3701 - val_loss: 7.0817\n",
      "Epoch 47/100\n",
      "3520/3520 [==============================] - 1s 150us/step - loss: 6.3693 - val_loss: 7.0795\n",
      "Epoch 48/100\n",
      "3520/3520 [==============================] - 1s 156us/step - loss: 6.3683 - val_loss: 7.0790\n",
      "Epoch 49/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3674 - val_loss: 7.0715\n",
      "Epoch 50/100\n",
      "3520/3520 [==============================] - 0s 123us/step - loss: 6.3668 - val_loss: 7.0754\n",
      "Epoch 51/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.3657 - val_loss: 7.0737\n",
      "Epoch 52/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.3653 - val_loss: 7.0735\n",
      "Epoch 53/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.3644 - val_loss: 7.0749\n",
      "Epoch 54/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.3638 - val_loss: 7.0710\n",
      "Epoch 55/100\n",
      "3520/3520 [==============================] - 0s 125us/step - loss: 6.3632 - val_loss: 7.0740\n",
      "Epoch 56/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.3627 - val_loss: 7.0748\n",
      "Epoch 57/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.3619 - val_loss: 7.0748\n",
      "Epoch 58/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3614 - val_loss: 7.0758\n",
      "Epoch 59/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3606 - val_loss: 7.0720\n",
      "Epoch 60/100\n",
      "3520/3520 [==============================] - 0s 123us/step - loss: 6.3601 - val_loss: 7.0724\n",
      "Epoch 61/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.3593 - val_loss: 7.0654\n",
      "Epoch 62/100\n",
      "3520/3520 [==============================] - 0s 125us/step - loss: 6.3586 - val_loss: 7.0691\n",
      "Epoch 63/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3579 - val_loss: 7.0654\n",
      "Epoch 64/100\n",
      "3520/3520 [==============================] - 0s 118us/step - loss: 6.3574 - val_loss: 7.0639\n",
      "Epoch 65/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.3567 - val_loss: 7.0668\n",
      "Epoch 66/100\n",
      "3520/3520 [==============================] - 0s 118us/step - loss: 6.3557 - val_loss: 7.0636\n",
      "Epoch 67/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3551 - val_loss: 7.0654\n",
      "Epoch 68/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3546 - val_loss: 7.0577\n",
      "Epoch 69/100\n",
      "3520/3520 [==============================] - 0s 123us/step - loss: 6.3540 - val_loss: 7.0637\n",
      "Epoch 70/100\n",
      "3520/3520 [==============================] - 0s 118us/step - loss: 6.3534 - val_loss: 7.0619\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.3528 - val_loss: 7.0552\n",
      "Epoch 72/100\n",
      "3520/3520 [==============================] - 0s 138us/step - loss: 6.3525 - val_loss: 7.0586\n",
      "Epoch 73/100\n",
      "3520/3520 [==============================] - 1s 143us/step - loss: 6.3522 - val_loss: 7.0504\n",
      "Epoch 74/100\n",
      "3520/3520 [==============================] - 1s 146us/step - loss: 6.3517 - val_loss: 7.0543\n",
      "Epoch 75/100\n",
      "3520/3520 [==============================] - 0s 128us/step - loss: 6.3514 - val_loss: 7.0564\n",
      "Epoch 76/100\n",
      "3520/3520 [==============================] - 0s 125us/step - loss: 6.3510 - val_loss: 7.0522\n",
      "Epoch 77/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.3508 - val_loss: 7.0532\n",
      "Epoch 78/100\n",
      "3520/3520 [==============================] - 0s 124us/step - loss: 6.3504 - val_loss: 7.0511\n",
      "Epoch 79/100\n",
      "3520/3520 [==============================] - 0s 126us/step - loss: 6.3501 - val_loss: 7.0517\n",
      "Epoch 80/100\n",
      "3520/3520 [==============================] - 0s 123us/step - loss: 6.3498 - val_loss: 7.0508\n",
      "Epoch 81/100\n",
      "3520/3520 [==============================] - 0s 121us/step - loss: 6.3498 - val_loss: 7.0530\n",
      "Epoch 82/100\n",
      "3520/3520 [==============================] - 0s 123us/step - loss: 6.3492 - val_loss: 7.0515\n",
      "Epoch 83/100\n",
      "3520/3520 [==============================] - 0s 123us/step - loss: 6.3491 - val_loss: 7.0487\n",
      "Epoch 84/100\n",
      "3520/3520 [==============================] - 0s 118us/step - loss: 6.3489 - val_loss: 7.0439\n",
      "Epoch 85/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.3487 - val_loss: 7.0499\n",
      "Epoch 86/100\n",
      "3520/3520 [==============================] - 0s 117us/step - loss: 6.3485 - val_loss: 7.0507\n",
      "Epoch 87/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.3483 - val_loss: 7.0499\n",
      "Epoch 88/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.3479 - val_loss: 7.0476\n",
      "Epoch 89/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3477 - val_loss: 7.0491\n",
      "Epoch 90/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3476 - val_loss: 7.0431\n",
      "Epoch 91/100\n",
      "3520/3520 [==============================] - 0s 119us/step - loss: 6.3472 - val_loss: 7.0404\n",
      "Epoch 92/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3470 - val_loss: 7.0436\n",
      "Epoch 93/100\n",
      "3520/3520 [==============================] - 0s 117us/step - loss: 6.3468 - val_loss: 7.0455\n",
      "Epoch 94/100\n",
      "3520/3520 [==============================] - 0s 117us/step - loss: 6.3463 - val_loss: 7.0442\n",
      "Epoch 95/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.3460 - val_loss: 7.0437\n",
      "Epoch 96/100\n",
      "3520/3520 [==============================] - 0s 120us/step - loss: 6.3459 - val_loss: 7.0429\n",
      "Epoch 97/100\n",
      "3520/3520 [==============================] - 0s 122us/step - loss: 6.3456 - val_loss: 7.0427\n",
      "Epoch 98/100\n",
      "3520/3520 [==============================] - 0s 132us/step - loss: 6.3455 - val_loss: 7.0438\n",
      "Epoch 99/100\n",
      "3520/3520 [==============================] - 1s 151us/step - loss: 6.3452 - val_loss: 7.0423\n",
      "Epoch 100/100\n",
      "3520/3520 [==============================] - 1s 147us/step - loss: 6.3450 - val_loss: 7.0404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = (encoder, decoder)\n",
    "\n",
    "data = (x_test, _)\n",
    "\n",
    "reconstruction_loss = binary_crossentropy(inputs,outputs)\n",
    "\n",
    "reconstruction_loss *= original_dim\n",
    "\n",
    "kl_loss = beta*(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
    "\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "\n",
    "kl_loss *= -0.5\n",
    "\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "vae.summary()\n",
    "\n",
    "plot_model(vae,\n",
    "\n",
    "           to_file='vae_mlp.png',\n",
    "\n",
    "           show_shapes=True)\n",
    "\n",
    "vae.fit(x_train,\n",
    "\n",
    "        epochs=epochs,\n",
    "\n",
    "        batch_size=batch_size,\n",
    "\n",
    "        validation_data=(x_test, None))\n",
    "\n",
    "vae.save_weights('vae_mlp_mnist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_decoded_test  [[0.21949686 0.39069065 0.35306314 0.3586908  0.17460985 0.05299781\n",
      "  0.39320374 0.4730194  0.43587437 0.47516754 0.29491848 0.06851693\n",
      "  0.3079473  0.43171483 0.4790571  0.54296625 0.3847209  0.10007013\n",
      "  0.18027598 0.29249623 0.31607476 0.37266248 0.26379785 0.0660468\n",
      "  0.21857335 0.3524698  0.389866   0.46254516 0.28054103 0.08136403\n",
      "  0.16265479 0.25632587 0.27648884 0.45318702 0.30538774 0.08995534]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x576 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(models,\n",
    "\n",
    "             data,\n",
    "\n",
    "             batch_size=batch_size,\n",
    "             model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
