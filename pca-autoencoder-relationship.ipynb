{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(123)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(234)\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import decomposition\n",
    "import scipy\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Layer, InputSpec\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers, activations, initializers, constraints, Sequential\n",
    "from keras import backend as K\n",
    "from keras.constraints import UnitNorm, Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random multi-dimensional correlated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**. Set the dimension of the data.\n",
    "\n",
    "We set the dim small to clear understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dim = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.1.** Generate a positive definite symmetric matrix to be used as covariance to generate a random data.\n",
    "\n",
    "This is a matrix of size n_dim x n_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov = sklearn.datasets.make_spd_matrix(n_dim, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.2.** Generate a vector of mean for generating the random data.\n",
    "\n",
    "This is an np array of size n_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.random.normal(0, 0.1, n_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**. Generate the random data, `X`.\n",
    "\n",
    "The number of samples for `X` is set as `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "X = np.random.multivariate_normal(mu, cov, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4.** Split the data into train and test.\n",
    "\n",
    "We split the data into train and test. The test will be used to measure the improvement in Autoencoder after tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52681732, 0.62810095, 0.59284649, 0.52236607, 0.40995414],\n",
       "       [0.41440755, 0.55001423, 0.63635182, 0.57413981, 0.30587165],\n",
       "       [0.38378646, 0.14857921, 0.39851437, 0.34588993, 0.63374338],\n",
       "       ...,\n",
       "       [0.28371994, 0.4794476 , 0.48410551, 0.54245635, 0.59818998],\n",
       "       [0.50510169, 0.76450508, 0.73271896, 0.69792451, 0.34080318],\n",
       "       [0.3978237 , 0.50478323, 0.40141433, 0.67971132, 0.60012005]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA vs Single Layer Linear Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "\n",
    "pca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Single Layer Linear Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/inferno/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/inferno/virtualenvs/keras_tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d5a0d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_train_scaled.shape[1] #num of predictor variables, \n",
    "encoding_dim = 2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True) \n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and contrast the outputs.\n",
    "\n",
    "### 1. Tied Weights\n",
    "\n",
    "The weights on Encoder and Decoder are not the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights \n",
      " [[-0.33  0.6   0.73  0.1   0.42]\n",
      " [ 0.07  0.53 -0.86 -0.76 -0.13]]\n",
      "Decoder weights \n",
      " [[-0.29  0.48  0.01  0.03 -0.45]\n",
      " [-0.83  0.04 -0.81 -0.7  -0.48]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = np.round(autoencoder.layers[0].get_weights()[0], 2).T  # W in Figure 2.\n",
    "w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 2)  # W' in Figure 2.\n",
    "print('Encoder weights \\n', w_encoder)\n",
    "print('Decoder weights \\n', w_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Weight Orthogonality\n",
    "Unlike PCA weights, the weights on Encoder and Decoder are not orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_pca = pca.components_\n",
    "np.round(np.dot(w_pca, w_pca.T), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.188, -0.464],\n",
       "       [-0.464,  1.62 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(w_encoder, w_encoder.T), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.518, 0.447],\n",
       "       [0.447, 2.067]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(w_decoder, w_decoder.T), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Uncorrelated Features\n",
    "Unlike PCA features, i.e. Principal Scores, the Encoded features are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09899, -0.     ],\n",
       "       [-0.     ,  0.01549]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_features = pca.fit_transform(X_train_scaled)\n",
    "np.round(np.cov(pca_features.T), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded feature covariance\n",
      " [[ 0.01597029 -0.01332291]\n",
      " [-0.01332291  0.02536361]]\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[0].output)\n",
    "encoded_features = np.array(encoder_layer.predict(X_train_scaled))\n",
    "print('Encoded feature covariance\\n', np.cov(encoded_features.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Unit Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA weights norm, \n",
      " [1. 1.]\n",
      "Encoder weights norm, \n",
      " [1.1882    1.6198999]\n",
      "Decoder weights norm, \n",
      " [0.51799995 2.0670002 ]\n"
     ]
    }
   ],
   "source": [
    "print('PCA weights norm, \\n', np.sum(w_pca ** 2, axis = 1))\n",
    "print('Encoder weights norm, \\n', np.sum(w_encoder ** 2, axis = 1))\n",
    "print('Decoder weights norm, \\n', np.sum(w_decoder ** 2, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Reconstruction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.016214465828467216\n",
      "Test reconstrunction error\n",
      " 0.015835085602281116\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well-posed Autoencoder \n",
    "### Constraints for Autoencoder \n",
    "Optimizing Autoencoder using PCA principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 16\n",
    "input_dim = X_train_scaled.shape[1] #num of predictor variables, \n",
    "encoding_dim = 2\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Constraint: Tied weights\n",
    "\n",
    "Make decoder weights equal to encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseTied(Layer):\n",
    "    def __init__(self, units,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 tied_to=None,\n",
    "                 **kwargs):\n",
    "        self.tied_to = tied_to\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "                \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        if self.tied_to is not None:\n",
    "            self.kernel = K.transpose(self.tied_to.kernel)\n",
    "            self._non_trainable_weights.append(self.kernel)\n",
    "        else:\n",
    "            self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                          initializer=self.kernel_initializer,\n",
    "                                          name='kernel',\n",
    "                                          regularizer=self.kernel_regularizer,\n",
    "                                          constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = K.dot(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Bias=False for Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_tied_1 (DenseTied)     (None, 5)                 10        \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 12\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d549a20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True) \n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = False)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=3,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights\n",
      " [[-0.727  0.441  0.271  0.273  0.893]\n",
      " [ 0.795  0.238  0.591  0.518 -0.144]]\n",
      "Decoder weights\n",
      " [[-0.727  0.441  0.271  0.273  0.893]\n",
      " [ 0.795  0.238  0.591  0.518 -0.144]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = np.round(np.transpose(autoencoder.layers[0].get_weights()[0]), 3)\n",
    "w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 3)\n",
    "print('Encoder weights\\n', w_encoder)\n",
    "print('Decoder weights\\n', w_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.0351764407278913\n",
      "Test reconstrunction error\n",
      " 0.03327750645552829\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Bias=True for Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_tied_2 (DenseTied)     (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 17\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d99b358>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True) \n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights\n",
      " [[-0.52  -0.383  0.02  -0.053  0.755]\n",
      " [ 0.229  0.486  0.415  0.509  0.458]]\n",
      "Decoder weights\n",
      " [[-0.52  -0.383  0.02  -0.053  0.755]\n",
      " [ 0.229  0.486  0.415  0.509  0.458]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = np.round(np.transpose(autoencoder.layers[0].get_weights()[0]), 3)\n",
    "w_decoder = np.round(autoencoder.layers[1].get_weights()[1], 3)\n",
    "print('Encoder weights\\n', w_encoder)\n",
    "print('Decoder weights\\n', w_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder bias\n",
      " [0.007 0.009]\n",
      "Decoder bias\n",
      " [ 0.208 -0.061  0.058 -0.022  0.068]\n"
     ]
    }
   ],
   "source": [
    "b_encoder = np.round(np.transpose(autoencoder.layers[0].get_weights()[1]), 3)\n",
    "b_decoder = np.round(np.transpose(autoencoder.layers[1].get_weights()[0]), 3)\n",
    "print('Encoder bias\\n', b_encoder)\n",
    "print('Decoder bias\\n', b_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.007770288062285628\n",
      "Test reconstrunction error\n",
      " 0.00800676049487586\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Constraint: Weights orthogonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WeightsOrthogonalityConstraint (Constraint):\n",
    "    def __init__(self, encoding_dim, weightage = 1.0, axis = 0):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.weightage = weightage\n",
    "        self.axis = axis\n",
    "        \n",
    "    def weights_orthogonality(self, w):\n",
    "        if(self.axis==1):\n",
    "            w = K.transpose(w)\n",
    "        if(self.encoding_dim > 1):\n",
    "            m = K.dot(K.transpose(w), w) - K.eye(self.encoding_dim)\n",
    "            return self.weightage * K.sqrt(K.sum(K.square(m)))\n",
    "        else:\n",
    "            m = K.sum(w ** 2) - 1.\n",
    "            return m\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.weights_orthogonality(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Encoder weight orthogonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12dc05b38>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias=True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0)) \n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights dot product\n",
      " [[0.98 0.01]\n",
      " [0.01 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = autoencoder.layers[0].get_weights()[0]\n",
    "print('Encoder weights dot product\\n', np.round(np.dot(w_encoder.T, w_encoder), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.025062192894754042\n",
      "Test reconstrunction error\n",
      " 0.023373541879966882\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Encoder and Decoder Weight orthogonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12de50320>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias=True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0)) \n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=1))\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights dot product\n",
      " [[ 1.   -0.  ]\n",
      " [-0.    0.98]]\n",
      "Decoder weights dot product\n",
      " [[1.   0.01]\n",
      " [0.01 0.97]]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = autoencoder.layers[0].get_weights()[0]\n",
    "print('Encoder weights dot product\\n', np.round(np.dot(w_encoder.T, w_encoder), 2))\n",
    "\n",
    "w_decoder = autoencoder.layers[1].get_weights()[0]\n",
    "print('Decoder weights dot product\\n', np.round(np.dot(w_decoder, w_decoder.T), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Constraint: Uncorrelated Encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UncorrelatedFeaturesConstraint (Constraint):\n",
    "\n",
    "    def __init__(self, encoding_dim, weightage=1.0):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.weightage = weightage\n",
    "\n",
    "    def get_covariance(self, x):\n",
    "        x_centered_list = []\n",
    "\n",
    "        for i in range(self.encoding_dim):\n",
    "            x_centered_list.append(x[:, i] - K.mean(x[:, i]))\n",
    "\n",
    "        x_centered = tf.stack(x_centered_list)\n",
    "        covariance = K.dot(x_centered, K.transpose(x_centered)) / \\\n",
    "            tf.cast(x_centered.get_shape()[0], tf.float32)\n",
    "\n",
    "        return covariance\n",
    "\n",
    "    # Constraint penalty\n",
    "    def uncorrelated_feature(self, x):\n",
    "        if(self.encoding_dim <= 1):\n",
    "            return 0.0\n",
    "        else:\n",
    "            output = K.sum(K.square(\n",
    "                self.covariance - tf.math.multiply(self.covariance, K.eye(self.encoding_dim))))\n",
    "            return output\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.covariance = self.get_covariance(x)\n",
    "        return self.weightage * self.uncorrelated_feature(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12fa19dd8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias=True,\n",
    "                activity_regularizer=UncorrelatedFeaturesConstraint(encoding_dim, weightage=1.))\n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias=True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded feature covariance\n",
      " [[ 0.024 -0.001]\n",
      " [-0.001  0.005]]\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[0].output)\n",
    "encoded_features = np.array(encoder_layer.predict(X_train_scaled))\n",
    "print('Encoded feature covariance\\n', np.round(np.cov(encoded_features.T), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.023617195423149294\n",
      "Test reconstrunction error\n",
      " 0.021963542347521507\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Constraint: Unit Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Unit Norm constraint on Encoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_65 (Dense)             (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135662208>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_constraint=UnitNorm(axis=0)) \n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights norm, \n",
      " [1.    0.998]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = np.round(autoencoder.layers[0].get_weights()[0], 2).T  # W in Figure 2.\n",
    "print('Encoder weights norm, \\n', np.round(np.sum(w_encoder ** 2, axis = 1),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.011734672569848077\n",
      "Test reconstrunction error\n",
      " 0.01836805504328783\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Unit Norm constraint on both Encoding and Decoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_67 (Dense)             (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 5)                 15        \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13589eb70>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_constraint=UnitNorm(axis=0)) \n",
    "decoder = Dense(input_dim, activation=\"linear\", use_bias = True, kernel_constraint=UnitNorm(axis=1))\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights norm, \n",
      " [1.007 1.001]\n",
      "Decoder weights norm, \n",
      " [0.998 0.998]\n"
     ]
    }
   ],
   "source": [
    "w_encoder = np.round(autoencoder.layers[0].get_weights()[0], 2).T  # W in Figure 2.\n",
    "w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 2)  # W' in Figure 2.\n",
    "\n",
    "print('Encoder weights norm, \\n', np.round(np.sum(w_encoder ** 2, axis = 1),3))\n",
    "print('Decoder weights norm, \\n', np.round(np.sum(w_decoder ** 2, axis = 1),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.011778800550343131\n",
      "Test reconstrunction error\n",
      " 0.020571250123387792\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_tied_5 (DenseTied)     (None, 5)                 10        \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 12\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1304ba9e8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Dense(encoding_dim, activation=\"linear\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0), kernel_constraint=UnitNorm(axis=0)) \n",
    "decoder = DenseTied(input_dim, activation=\"linear\", tied_to=encoder, use_bias = False)\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='sgd')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reconstrunction error\n",
      " 0.007643200001013366\n",
      "Test reconstrunction error\n",
      " 0.013138687548721828\n"
     ]
    }
   ],
   "source": [
    "train_predictions = autoencoder.predict(X_train_scaled)\n",
    "print('Train reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))\n",
    "test_predictions = autoencoder.predict(X_test_scaled)\n",
    "print('Test reconstrunction error\\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf",
   "language": "python",
   "name": "keras_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
